General Multi-label Image Classification with Transformers

---------------

### *基于Transformer的普遍多标签分类*

--------------

##### 摘要

--------

1. 利用C-Tran通用分类框架进行多标签图像分类。
2. 利用标签掩码训练目标（标签的三种状态：正P、负N、未知U）。
3. 在COCO和Visual Genome数据集上表现良好。
4. **尤其注意的是允许带有部分以及额外标签注释的图像生成改进结果，同样在上述数据集中展现了这种能力。**
5. 值得注意的是，本文提到的transformer是指**Transformer**是谷歌在2017年发布的一个用来替代RNN和CNN的新的网络结构，Transformer本质上就是一个Attention结构，它能够直接获取全局的信息，而不像RNN需要逐步递归才能获得全局信息，也不像CNN只能获取局部信息，并且其能够进行并行运算，要比RNN快上很多倍。
6. 该文在Transformer的基础上，提出了分类Transformer (C-Tran)。

##### 核心

---------

1. C-Tran架构中，人物、雨伞、太阳镜标签被随机遮盖，用作未知标签，损失函数仅仅在未知标签中进行预测计算。

   - 首先图像进行Resnet处理形成Zi，进行feature embedding特征嵌入。之后进行标签嵌入，最后也是最具有特点的事进行状态嵌入。
   - 下一步进行打标签的操作，加入状态N\P\U,进行编码，

   

   

![image-20220511163209897](C:/Users/LEGION/AppData/Roaming/Typora/typora-user-images/image-20220511163209897.png)

2. 步骤

   - 特征、标签和状态嵌入（ Feature, Label, and State Embeddings）：传统架构没有编码机制，其将部分已知或额外的标签作为模型注入，加大计算量和性能需求。

   > 为了解决这一问题，提出了多标签图像识别
   >
   > li = li + si
   >
   > 标签为P：标签中存在一些推理之前所知道的正值；
   >
   > 标签为N：未知；
   >
   > 标签为U：固定全0；

   - 特征、标签和状态嵌入（Modeling Feature and Label Interactions with a Transformer Encoder）：标签嵌⼊联合输⼊到 Transformer编码器。
   - 标签推理分类器（Label Inference Classifie）：Transformer 编码器会对特征和标签依赖关系进行建模。
   - 标签掩码训练（MIT）：状态嵌⼊（等式1）让我们可以轻松地将已知标签作为 C-Tran 的输入。强制模型学习标签相关性，并允许C-Tran泛化到任何推理设置。

3. C-Tran这是一种用于多标签图像分类的新颖灵活的深度学习模型。 我们的方法易于实现，并且可以在推理过程中有效地利用任意一组部分或额外的标签。CTran通过注意力学习样本自适应交互，并发现标签如何处理图像的不同部分。 我们展示了我们的方法在常规多标签分类和具有部分观察或额外标签的多标签分类中的有效性。C-Tran 在广泛的场景中优于最先进的方法。我们进一步提供定量和定性分析，表明。C-Tran 通过显式建模目标标签之间以及图像特征与目标之间的交互来获得收益。

4. 全篇方法概述：我们的方法包括一个经过训练的 Transformer 编码器，用于在给定一组掩码标签输入的情况下预测一组目标标签，以及来自卷积神经网络的视觉特征。我们方法的一个关键组成部分是标签掩码训练目标，它使用三元编码方案在训练期间将标签的状态表示为正、负或未知。

#### 实验结果

-------------

1. 阈值设置为 0.5 以计算精度、召回率和 F1 分数 (%)。我们的模型在 COCO 和 Visual Genome 等具有挑战性的数据集上显示了最先进的性能。此外，由于我们的模型在训练期间明确表示了标签状态，因此通过允许我们在推理期间为具有部分或额外标签注释的图像产生改进的结果，它更通用。我们在 COCO、Visual Genome、News-500 和 CUB 图像数据集中展示了这种附加功能。

![](https://gitee.com/datie666/images/raw/master/img/202205111741312.png)



